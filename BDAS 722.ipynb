{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Business and/or Situation understanding\n",
    "\n",
    "#The objective of this research is to identify clusters of factors that contribute \n",
    "#to high probability of death or injury in a motor vehicle. \n",
    "#Prepared by Michael Gerlikhman, InfoSys722.\n",
    "#GitHub: https://github.com/Gerlikhman/INFOSYS722_BDAS\n",
    "\n",
    "#Initiation and Import\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "spark = SparkSession.builder.appName('Contributing_Factors_to_Motor_Vehicle_Crashes_in_New_Zealand').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Data understanding\n",
    "\n",
    "#Loading the datasets\n",
    "crash = spark.read.load('finaldata_201809.csv', format = 'csv', header = 'true')\n",
    "driver = spark.read.load('driverdata_simulated.csv', format = 'csv', header = 'true')\n",
    "\n",
    "#Tableau Was Used for initial visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|           HOLIDAY|           MULTI_VEH|            TLA_NAME|\n",
      "+------------------+--------------------+--------------------+\n",
      "|Christmas/New Year|      Single vehicle|       Tauranga City|\n",
      "|   Queens Birthday|Vehicle(s)+Cyclis...|        Dunedin City|\n",
      "|Christmas/New Year|Vehicle(s)+Pedest...|   Christchurch City|\n",
      "|Christmas/New Year|       Multi vehicle|     Clutha District|\n",
      "|    Labour Weekend|       Multi vehicle|  Whakatane District|\n",
      "|              None|Others without no...|     Wellington City|\n",
      "|              None|Vehicle(s)+Cyclis...|     Clutha District|\n",
      "|    Labour Weekend|Vehicle(s)+Pedest...|Waimakariri District|\n",
      "|Christmas/New Year|Vehicle(s)+Pedest...|     Wellington City|\n",
      "|    Labour Weekend|      Single vehicle|         Nelson City|\n",
      "|              None|       Multi vehicle|     Timaru District|\n",
      "|   Queens Birthday|      Single vehicle|  Whangarei District|\n",
      "|Christmas/New Year|       Multi vehicle|    Opotiki District|\n",
      "|    Labour Weekend|Vehicle(s)+Cyclis...|       Tauranga City|\n",
      "|            Easter|Vehicle(s)+Pedest...|New Plymouth Dist...|\n",
      "|              None|       Multi vehicle|   Manawatu District|\n",
      "|            Easter|      Single vehicle|Kapiti Coast Dist...|\n",
      "|    Labour Weekend|      Single vehicle|  Southland District|\n",
      "|    Labour Weekend|      Single vehicle|   Wanganui District|\n",
      "|Christmas/New Year|Vehicle(s)+Pedest...|         Napier City|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View Unique values in categorical variables\n",
    "crash.createOrReplaceTempView('crash')\n",
    "\n",
    "#Preview\n",
    "spark.sql(\"\"\"SELECT distinct\n",
    "        HOLIDAY, MULTI_VEH, TLA_NAME      \n",
    "    FROM\n",
    "        crash\n",
    "    \"\"\").show() \n",
    "\n",
    "#Full View (optional)\n",
    "#spark.sql(\"\"\"SELECT distinct\n",
    "#        HOLIDAY, MULTI_VEH, TLA_NAME, CRASH_LOCN1, INTERSECTION, JUNCTION_TYPE, INTSN_MIDBLOCK,\n",
    "#        FLAT_HILL, ROAD_CHARACTER, ROAD_CURVATURE, ROAD_SURFACE, LIGHT, WEATHER_A, WEATHER_B       \n",
    "#    FROM\n",
    "#        crash\n",
    "#    \"\"\").show(20, False) \n",
    "\n",
    "#crash.select(\"TLA_NAME\").distinct().show() #exploring unique values for a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRASH_YEAR: string (nullable = true)\n",
      " |-- CRASH_FIN_YEAR: string (nullable = true)\n",
      " |-- CRASH_SEV: string (nullable = true)\n",
      " |-- FATAL_COUNT: string (nullable = true)\n",
      " |-- SERIOUSINJ_COUNT: string (nullable = true)\n",
      " |-- MINORINJ_COUNT: string (nullable = true)\n",
      " |-- MULTI_VEH: string (nullable = true)\n",
      " |-- HOLIDAY: string (nullable = true)\n",
      " |-- LG_REGION_DESC: string (nullable = true)\n",
      " |-- TLA_ID: string (nullable = true)\n",
      " |-- TLA_NAME: string (nullable = true)\n",
      " |-- AU_ID: string (nullable = true)\n",
      " |-- MB_ID: string (nullable = true)\n",
      " |-- EASTING: string (nullable = true)\n",
      " |-- NORTHING: string (nullable = true)\n",
      " |-- CRASH_LOCN1: string (nullable = true)\n",
      " |-- CRASH_LOCN2: string (nullable = true)\n",
      " |-- OUTDTD_LOCN_DESC: string (nullable = true)\n",
      " |-- CRASH_RP_RS: string (nullable = true)\n",
      " |-- INTERSECTION: string (nullable = true)\n",
      " |-- JUNCTION_TYPE: string (nullable = true)\n",
      " |-- CR_RD_SIDE_RD: string (nullable = true)\n",
      " |-- CRASH_DIRN_DESC: string (nullable = true)\n",
      " |-- CRASH_DIST: string (nullable = true)\n",
      " |-- CRASH_RP_DIRN_DESC: string (nullable = true)\n",
      " |-- DIRN_ROLE1_DESC: string (nullable = true)\n",
      " |-- CRASH_RP_DISP: string (nullable = true)\n",
      " |-- CRASH_SH_DESC: string (nullable = true)\n",
      " |-- CRASH_RP_SH: string (nullable = true)\n",
      " |-- CRASH_RP_NEWS_DESC: string (nullable = true)\n",
      " |-- INTSN_MIDBLOCK: string (nullable = true)\n",
      " |-- FLAT_HILL: string (nullable = true)\n",
      " |-- ROAD_CHARACTER: string (nullable = true)\n",
      " |-- ROAD_CURVATURE: string (nullable = true)\n",
      " |-- ROAD_LANE: string (nullable = true)\n",
      " |-- ROAD_MARKINGS: string (nullable = true)\n",
      " |-- ROAD_SURFACE: string (nullable = true)\n",
      " |-- ROAD_WET: string (nullable = true)\n",
      " |-- NUM_LANES: string (nullable = true)\n",
      " |-- TRAFFIC_CTRL: string (nullable = true)\n",
      " |-- SPD_LIM: string (nullable = true)\n",
      " |-- ADV_SPD: string (nullable = true)\n",
      " |-- TMP_SPD_LIM: string (nullable = true)\n",
      " |-- URBAN: string (nullable = true)\n",
      " |-- DARK_LIGHT: string (nullable = true)\n",
      " |-- LIGHT: string (nullable = true)\n",
      " |-- STREET_LIGHT: string (nullable = true)\n",
      " |-- WEATHER_A: string (nullable = true)\n",
      " |-- WEATHER_B: string (nullable = true)\n",
      " |-- ANIMALS: string (nullable = true)\n",
      " |-- BRIDGE: string (nullable = true)\n",
      " |-- CLIFF_BANK: string (nullable = true)\n",
      " |-- DEBRIS: string (nullable = true)\n",
      " |-- DITCH: string (nullable = true)\n",
      " |-- FENCE: string (nullable = true)\n",
      " |-- GUARD_RAIL: string (nullable = true)\n",
      " |-- HOUSE_OR_BLDG: string (nullable = true)\n",
      " |-- KERB: string (nullable = true)\n",
      " |-- OBJ_THROWN_DROPPED: string (nullable = true)\n",
      " |-- OTHER: string (nullable = true)\n",
      " |-- OVER_BANK: string (nullable = true)\n",
      " |-- PARKED_VEHICLE: string (nullable = true)\n",
      " |-- PHONE_BOX_ETC: string (nullable = true)\n",
      " |-- POST_OR_POLE: string (nullable = true)\n",
      " |-- ROADWORKS: string (nullable = true)\n",
      " |-- SLIP_OR_FLOOD: string (nullable = true)\n",
      " |-- STRAY_ANIMAL: string (nullable = true)\n",
      " |-- TRAFFIC_ISLAND: string (nullable = true)\n",
      " |-- TRAFFIC_SIGN: string (nullable = true)\n",
      " |-- TRAIN: string (nullable = true)\n",
      " |-- TREE: string (nullable = true)\n",
      " |-- VEHICLE: string (nullable = true)\n",
      " |-- WATER_RIVER: string (nullable = true)\n",
      " |-- BICYCLE: string (nullable = true)\n",
      " |-- BUS: string (nullable = true)\n",
      " |-- CAR_STN_WAGON: string (nullable = true)\n",
      " |-- MOPED: string (nullable = true)\n",
      " |-- MOTOR_CYCLE: string (nullable = true)\n",
      " |-- OTHER_VEHICLE_TYPE: string (nullable = true)\n",
      " |-- SCHOOL_BUS: string (nullable = true)\n",
      " |-- SUV: string (nullable = true)\n",
      " |-- TAXI: string (nullable = true)\n",
      " |-- TRUCK: string (nullable = true)\n",
      " |-- UNKNOWN_VEHICLE_TYPE: string (nullable = true)\n",
      " |-- VAN_OR_UTILITY: string (nullable = true)\n",
      " |-- PEDESTRIAN: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data Exploration\n",
    "\n",
    "#crash.count()\n",
    "crash.printSchema() #all strings?\n",
    "\n",
    "#crash.describe().show() #very slow with large dataset, not practical\n",
    "#crash.show() #too large to display in a practical way\n",
    "\n",
    "#merged.select(\"TLA_NAME\").distinct().show() #exploring unique values for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Irrelevant Columns (based on meta description)\n",
    "\n",
    "crash = crash.drop(*['CRASH_FIN_YEAR','LG_REGION_DESC','TLA_ID','AU_ID', 'MB_ID', \n",
    "                     'EASTING','NORTHING','CRASH_LOCN2', 'OUTDTD_LOCN_DESC',\n",
    "                     'CRASH_RP_RS','CR_RD_SIDE_RD','CRASH_DIRN_DESC','CRASH_DIST',\n",
    "                     'CRASH_RP_DIRN_DESC','DIRN_ROLE1_DESC','CRASH_RP_DISP', 'TRAFFIC_CTRL',\n",
    "                     'CRASH_SH_DESC','CRASH_RP_SH','CRASH_RP_NEWS_DESC', 'ROAD_WET', 'URBAN',\n",
    "                     'ROAD_LANE','DARK_LIGHT','OTHER','OTHER_VEHICLE_TYPE','UNKNOWN_VEHICLE_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|   Sex|  Age|DUI|\n",
      "+------+-----+---+\n",
      "|Female|20-24|  F|\n",
      "|Female|60-64|  F|\n",
      "|  Male|40-44|  F|\n",
      "|  Male|15-19|  T|\n",
      "|Female|35-39|  F|\n",
      "|Female|15-19|  T|\n",
      "|  Male|45-49|  F|\n",
      "|  Male|30-34|  F|\n",
      "|  Male|35-39|  F|\n",
      "|  Male|25-29|  F|\n",
      "|  Male|45-49|  F|\n",
      "|Female|65-69|  F|\n",
      "|  Male|40-44|  T|\n",
      "|Female|40-44|  T|\n",
      "|  Male|45-49|  T|\n",
      "|  Male|  80+|  F|\n",
      "|  Male|30-34|  F|\n",
      "|  Male|30-34|  F|\n",
      "|  Male|65-69|  F|\n",
      "|Female|  80+|  F|\n",
      "+------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data Exploration and Drop for Driver Set\n",
    "\n",
    "driver.columns\n",
    "driver = driver.drop(*['_c0', 'CRASH_YEAR', 'CRASH_SEV']) #these were a part of simulation, and are now duplicate\n",
    "driver.show() #remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortunately there is no 'easy' (at least for me) method to combine the two very large dataframes \n",
    "#on index or id in spark due to partitioning features, when applied this method \n",
    "#is unreliable  and outputs only half of the original rows (partitions > 1 are ignored). \n",
    "#I have left my attempts here for the record.\n",
    "\n",
    "#from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#crash = crash.withColumn('id', monotonically_increasing_id())\n",
    "#driver = driver.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "#merged = crash.join(driver, crash.id == driver.id)\n",
    "#or\n",
    "#merged_data = crash.join(driver, 'id', 'inner').drop('id')\n",
    "#or\n",
    "#merged = crash.join(driver, on=['row_index'])\n",
    "\n",
    "#merged_data.count()\n",
    "#reports only 332963 rows when 674811 are expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a merged (elsewhere) dataset\n",
    "\n",
    "#Adding schema to the dataset\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,BooleanType,StructType)\n",
    "\n",
    "data_schema = [StructField('_c0',IntegerType(),True),\n",
    "              StructField('CRASH_YEAR',StringType(),True),\n",
    "              StructField('CRASH_SEV',StringType(),True),\n",
    "              StructField('FATAL_COUNT',IntegerType(),True),\n",
    "              StructField('SERIOUSINJ_COUNT',IntegerType(),True),\n",
    "              StructField('MINORINJ_COUNT',IntegerType(),True),\n",
    "              StructField('MULTI_VEH',StringType(),True),\n",
    "              StructField('HOLIDAY',StringType(),True),\n",
    "              StructField('TLA_NAME',StringType(),True),\n",
    "              StructField('CRASH_LOCN1',StringType(),True),\n",
    "              StructField('INTERSECTION',StringType(),True),\n",
    "              StructField('JUNCTION_TYPE',StringType(),True),\n",
    "              StructField('INTSN_MIDBLOCK',StringType(),True),\n",
    "              StructField('FLAT_HILL',StringType(),True),\n",
    "              StructField('ROAD_CHARACTER',StringType(),True), \n",
    "              StructField('ROAD_CURVATURE',StringType(),True), \n",
    "              StructField('ROAD_MARKINGS',StringType(),True),\n",
    "              StructField('ROAD_SURFACE',StringType(),True),\n",
    "              StructField('NUM_LANES',StringType(),True),\n",
    "              StructField('SPD_LIM',StringType(),True),\n",
    "              StructField('ADV_SPD',StringType(),True),\n",
    "              StructField('TMP_SPD_LIM',StringType(),True),\n",
    "              StructField('LIGHT',StringType(),True),\n",
    "              StructField('STREET_LIGHT',StringType(),True),\n",
    "              StructField('WEATHER_A',StringType(),True),\n",
    "              StructField('WEATHER_B',StringType(),True),\n",
    "              StructField('ANIMALS',IntegerType(),True),\n",
    "              StructField('BRIDGE',IntegerType(),True),\n",
    "              StructField('CLIFF_BANK',IntegerType(),True),\n",
    "              StructField('DEBRIS',IntegerType(),True),\n",
    "              StructField('DITCH',IntegerType(),True),\n",
    "              StructField('FENCE',IntegerType(),True),\n",
    "              StructField('GUARD_RAIL',IntegerType(),True),\n",
    "              StructField('HOUSE_OR_BLDG',IntegerType(),True),\n",
    "              StructField('KERB',IntegerType(),True),\n",
    "              StructField('OBJ_THROWN_DROPPED',IntegerType(),True),\n",
    "              StructField('OVER_BANK',IntegerType(),True),\n",
    "              StructField('PARKED_VEHICLE',IntegerType(),True),\n",
    "              StructField('PHONE_BOX_ETC',IntegerType(),True),\n",
    "              StructField('POST_OR_POLE',IntegerType(),True),\n",
    "              StructField('ROADWORKS',IntegerType(),True),\n",
    "              StructField('SLIP_OR_FLOOD',IntegerType(),True),\n",
    "              StructField('STRAY_ANIMAL',IntegerType(),True),\n",
    "              StructField('TRAFFIC_ISLAND',IntegerType(),True),\n",
    "              StructField('TRAFFIC_SIGN',IntegerType(),True),\n",
    "              StructField('TRAIN',IntegerType(),True),\n",
    "              StructField('TREE',IntegerType(),True),\n",
    "              StructField('VEHICLE',IntegerType(),True),\n",
    "              StructField('WATER_RIVER',IntegerType(),True),\n",
    "              StructField('BICYCLE',IntegerType(),True),\n",
    "              StructField('BUS',IntegerType(),True),\n",
    "              StructField('CAR_STN_WAGON',IntegerType(),True),\n",
    "              StructField('MOPED',IntegerType(),True),\n",
    "              StructField('MOTOR_CYCLE',IntegerType(),True),\n",
    "              StructField('SCHOOL_BUS',IntegerType(),True),\n",
    "              StructField('SUV',IntegerType(),True),\n",
    "              StructField('TAXI',IntegerType(),True),\n",
    "              StructField('TRUCK',IntegerType(),True),\n",
    "              StructField('VAN_OR_UTILITY',IntegerType(),True),\n",
    "              StructField('PEDESTRIAN',IntegerType(),True),\n",
    "              StructField('Sex',StringType(),True),\n",
    "              StructField('Age',StringType(),True), \n",
    "              StructField('DUI',StringType(),True)]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)\n",
    "\n",
    "merged = spark.read.load('merged.csv', format = 'csv', header = 'true', schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- CRASH_YEAR: string (nullable = true)\n",
      " |-- CRASH_SEV: string (nullable = true)\n",
      " |-- FATAL_COUNT: integer (nullable = true)\n",
      " |-- SERIOUSINJ_COUNT: integer (nullable = true)\n",
      " |-- MINORINJ_COUNT: integer (nullable = true)\n",
      " |-- MULTI_VEH: string (nullable = true)\n",
      " |-- HOLIDAY: string (nullable = true)\n",
      " |-- TLA_NAME: string (nullable = true)\n",
      " |-- CRASH_LOCN1: string (nullable = true)\n",
      " |-- INTERSECTION: string (nullable = true)\n",
      " |-- JUNCTION_TYPE: string (nullable = true)\n",
      " |-- INTSN_MIDBLOCK: string (nullable = true)\n",
      " |-- FLAT_HILL: string (nullable = true)\n",
      " |-- ROAD_CHARACTER: string (nullable = true)\n",
      " |-- ROAD_CURVATURE: string (nullable = true)\n",
      " |-- ROAD_MARKINGS: string (nullable = true)\n",
      " |-- ROAD_SURFACE: string (nullable = true)\n",
      " |-- NUM_LANES: string (nullable = true)\n",
      " |-- SPD_LIM: string (nullable = true)\n",
      " |-- ADV_SPD: string (nullable = true)\n",
      " |-- TMP_SPD_LIM: string (nullable = true)\n",
      " |-- LIGHT: string (nullable = true)\n",
      " |-- STREET_LIGHT: string (nullable = true)\n",
      " |-- WEATHER_A: string (nullable = true)\n",
      " |-- WEATHER_B: string (nullable = true)\n",
      " |-- ANIMALS: integer (nullable = true)\n",
      " |-- BRIDGE: integer (nullable = true)\n",
      " |-- CLIFF_BANK: integer (nullable = true)\n",
      " |-- DEBRIS: integer (nullable = true)\n",
      " |-- DITCH: integer (nullable = true)\n",
      " |-- FENCE: integer (nullable = true)\n",
      " |-- GUARD_RAIL: integer (nullable = true)\n",
      " |-- HOUSE_OR_BLDG: integer (nullable = true)\n",
      " |-- KERB: integer (nullable = true)\n",
      " |-- OBJ_THROWN_DROPPED: integer (nullable = true)\n",
      " |-- OVER_BANK: integer (nullable = true)\n",
      " |-- PARKED_VEHICLE: integer (nullable = true)\n",
      " |-- PHONE_BOX_ETC: integer (nullable = true)\n",
      " |-- POST_OR_POLE: integer (nullable = true)\n",
      " |-- ROADWORKS: integer (nullable = true)\n",
      " |-- SLIP_OR_FLOOD: integer (nullable = true)\n",
      " |-- STRAY_ANIMAL: integer (nullable = true)\n",
      " |-- TRAFFIC_ISLAND: integer (nullable = true)\n",
      " |-- TRAFFIC_SIGN: integer (nullable = true)\n",
      " |-- TRAIN: integer (nullable = true)\n",
      " |-- TREE: integer (nullable = true)\n",
      " |-- VEHICLE: integer (nullable = true)\n",
      " |-- WATER_RIVER: integer (nullable = true)\n",
      " |-- BICYCLE: integer (nullable = true)\n",
      " |-- BUS: integer (nullable = true)\n",
      " |-- CAR_STN_WAGON: integer (nullable = true)\n",
      " |-- MOPED: integer (nullable = true)\n",
      " |-- MOTOR_CYCLE: integer (nullable = true)\n",
      " |-- SCHOOL_BUS: integer (nullable = true)\n",
      " |-- SUV: integer (nullable = true)\n",
      " |-- TAXI: integer (nullable = true)\n",
      " |-- TRUCK: integer (nullable = true)\n",
      " |-- VAN_OR_UTILITY: integer (nullable = true)\n",
      " |-- PEDESTRIAN: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- DUI: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged.printSchema() #now types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674811"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.count() #correct number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665055"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.2 Data Clean\n",
    "#Filtering out NA's and some Unknown observations (not all, as some geniuinly describe a crash)\n",
    "filtered_data = merged.filter((merged.FLAT_HILL != 'Unknown') &\n",
    "                                   (merged.ROAD_CURVATURE != 'Unknown') & \n",
    "                                   (merged.ROAD_SURFACE != 'Unknown') & \n",
    "                                   (merged.LIGHT != 'Unknown') &\n",
    "                                   (merged.WEATHER_A != 'Unknown') &\n",
    "                                   (merged.SPD_LIM != ''))\n",
    "\n",
    "filtered_data.na.fill('None')\n",
    "\n",
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for any NAs (optional, takes a while)\n",
    "\n",
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "#filtered_data.select([count(when(isnan(c), c)).alias(c) for c in filtered_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Response Varialbes\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "#Creating a count variable that is the sum of fatal and serious injuries\n",
    "filtered_data = filtered_data.withColumn('FS',filtered_data['SERIOUSINJ_COUNT']+filtered_data['FATAL_COUNT'])\n",
    "\n",
    "#Creating a flag variable if crash had any fatal and serious injuries\n",
    "filtered_data = filtered_data.withColumn('FSL',when(filtered_data.FS > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT (save) *OPTIONAL\n",
    "#filtered_data.repartition(1).write.format('com.databricks.spark.csv').save(\"filtered_save\",header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT (load) *OPTIONAL\n",
    "\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,BooleanType,StructType)\n",
    "\n",
    "data_schema2 = [StructField('_c0',IntegerType(),True),\n",
    "              StructField('CRASH_YEAR',StringType(),True),\n",
    "              StructField('CRASH_SEV',StringType(),True),\n",
    "              StructField('FATAL_COUNT',IntegerType(),True),\n",
    "              StructField('SERIOUSINJ_COUNT',IntegerType(),True),\n",
    "              StructField('MINORINJ_COUNT',IntegerType(),True),\n",
    "              StructField('MULTI_VEH',StringType(),True),\n",
    "              StructField('HOLIDAY',StringType(),True),\n",
    "              StructField('TLA_NAME',StringType(),True),\n",
    "              StructField('CRASH_LOCN1',StringType(),True),\n",
    "              StructField('INTERSECTION',StringType(),True),\n",
    "              StructField('JUNCTION_TYPE',StringType(),True),\n",
    "              StructField('INTSN_MIDBLOCK',StringType(),True),\n",
    "              StructField('FLAT_HILL',StringType(),True),\n",
    "              StructField('ROAD_CHARACTER',StringType(),True), \n",
    "              StructField('ROAD_CURVATURE',StringType(),True), \n",
    "              StructField('ROAD_MARKINGS',StringType(),True),\n",
    "              StructField('ROAD_SURFACE',StringType(),True),\n",
    "              StructField('NUM_LANES',StringType(),True),\n",
    "              StructField('SPD_LIM',StringType(),True),\n",
    "              StructField('ADV_SPD',StringType(),True),\n",
    "              StructField('TMP_SPD_LIM',StringType(),True),\n",
    "              StructField('LIGHT',StringType(),True),\n",
    "              StructField('STREET_LIGHT',StringType(),True),\n",
    "              StructField('WEATHER_A',StringType(),True),\n",
    "              StructField('WEATHER_B',StringType(),True),\n",
    "              StructField('ANIMALS',IntegerType(),True),\n",
    "              StructField('BRIDGE',IntegerType(),True),\n",
    "              StructField('CLIFF_BANK',IntegerType(),True),\n",
    "              StructField('DEBRIS',IntegerType(),True),\n",
    "              StructField('DITCH',IntegerType(),True),\n",
    "              StructField('FENCE',IntegerType(),True),\n",
    "              StructField('GUARD_RAIL',IntegerType(),True),\n",
    "              StructField('HOUSE_OR_BLDG',IntegerType(),True),\n",
    "              StructField('KERB',IntegerType(),True),\n",
    "              StructField('OBJ_THROWN_DROPPED',IntegerType(),True),\n",
    "              StructField('OVER_BANK',IntegerType(),True),\n",
    "              StructField('PARKED_VEHICLE',IntegerType(),True),\n",
    "              StructField('PHONE_BOX_ETC',IntegerType(),True),\n",
    "              StructField('POST_OR_POLE',IntegerType(),True),\n",
    "              StructField('ROADWORKS',IntegerType(),True),\n",
    "              StructField('SLIP_OR_FLOOD',IntegerType(),True),\n",
    "              StructField('STRAY_ANIMAL',IntegerType(),True),\n",
    "              StructField('TRAFFIC_ISLAND',IntegerType(),True),\n",
    "              StructField('TRAFFIC_SIGN',IntegerType(),True),\n",
    "              StructField('TRAIN',IntegerType(),True),\n",
    "              StructField('TREE',IntegerType(),True),\n",
    "              StructField('VEHICLE',IntegerType(),True),\n",
    "              StructField('WATER_RIVER',IntegerType(),True),\n",
    "              StructField('BICYCLE',IntegerType(),True),\n",
    "              StructField('BUS',IntegerType(),True),\n",
    "              StructField('CAR_STN_WAGON',IntegerType(),True),\n",
    "              StructField('MOPED',IntegerType(),True),\n",
    "              StructField('MOTOR_CYCLE',IntegerType(),True),\n",
    "              StructField('SCHOOL_BUS',IntegerType(),True),\n",
    "              StructField('SUV',IntegerType(),True),\n",
    "              StructField('TAXI',IntegerType(),True),\n",
    "              StructField('TRUCK',IntegerType(),True),\n",
    "              StructField('VAN_OR_UTILITY',IntegerType(),True),\n",
    "              StructField('PEDESTRIAN',IntegerType(),True),\n",
    "              StructField('Sex',StringType(),True),\n",
    "              StructField('Age',StringType(),True), \n",
    "              StructField('DUI',StringType(),True),\n",
    "              StructField('FS',IntegerType(),True),\n",
    "              StructField('FSL',IntegerType(),True)]\n",
    "\n",
    "final_struct2 = StructType(fields=data_schema2)\n",
    "\n",
    "\n",
    "#df = spark.read.load('filtered.csv', format = 'csv', header = 'true', schema=final_struct2) #load from checkpoint if needed\n",
    "#or\n",
    "df = filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-----+---+\n",
      "| FS|FSL|   Sex|  Age|DUI|\n",
      "+---+---+------+-----+---+\n",
      "|  3|  1|Female|20-24|  F|\n",
      "|  2|  1|Female|60-64|  F|\n",
      "|  3|  1|  Male|40-44|  F|\n",
      "|  6|  1|  Male|15-19|  T|\n",
      "|  1|  1|Female|35-39|  F|\n",
      "|  1|  1|Female|15-19|  T|\n",
      "|  1|  1|  Male|45-49|  F|\n",
      "|  1|  1|  Male|30-34|  F|\n",
      "|  1|  1|  Male|35-39|  F|\n",
      "|  2|  1|  Male|25-29|  F|\n",
      "|  1|  1|  Male|45-49|  F|\n",
      "|  3|  1|Female|65-69|  F|\n",
      "|  1|  1|  Male|40-44|  T|\n",
      "|  1|  1|Female|40-44|  T|\n",
      "|  1|  1|  Male|45-49|  T|\n",
      "|  2|  1|  Male|  80+|  F|\n",
      "|  1|  1|  Male|30-34|  F|\n",
      "|  2|  1|  Male|30-34|  F|\n",
      "|  1|  1|  Male|65-69|  F|\n",
      "|  1|  1|Female|  80+|  F|\n",
      "+---+---+------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6.1 Conduct exploratory analysis and discuss\n",
    "\n",
    "#Creating a small (10k) test subset for algorithm selection\n",
    "df_t = df.limit(10000).drop('_c0')\n",
    "#df_t.columns\n",
    "\n",
    "responce_vars = ['FS','FSL']\n",
    "variable_list_emblem = ['Sex','Age','DUI']\n",
    "df_t = df_t.select(responce_vars + variable_list_emblem)\n",
    "\n",
    "df_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data indexing -> encoding -> assembly pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "categorical_columns= ['Sex', 'Age', 'DUI']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "#Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "\n",
    "#Call to transform\n",
    "model_t = pipeline.fit(df_t)\n",
    "transformed_t = model_t.transform(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+\n",
      "| FS|FSL|            features|\n",
      "+---+---+--------------------+\n",
      "|  3|  1|(18,[1,2,16],[1.0...|\n",
      "|  2|  1|(18,[1,11,16],[1....|\n",
      "|  3|  1|(18,[0,6,16],[1.0...|\n",
      "|  6|  1|(18,[0,4,17],[1.0...|\n",
      "|  1|  1|(18,[1,5,16],[1.0...|\n",
      "|  1|  1|(18,[1,4,17],[1.0...|\n",
      "|  1|  1|(18,[0,8,16],[1.0...|\n",
      "|  1|  1|(18,[0,7,16],[1.0...|\n",
      "|  1|  1|(18,[0,5,16],[1.0...|\n",
      "|  2|  1|(18,[0,3,16],[1.0...|\n",
      "|  1|  1|(18,[0,8,16],[1.0...|\n",
      "|  3|  1|(18,[1,13,16],[1....|\n",
      "|  1|  1|(18,[0,6,17],[1.0...|\n",
      "|  1|  1|(18,[1,6,17],[1.0...|\n",
      "|  1|  1|(18,[0,8,17],[1.0...|\n",
      "|  2|  1|(18,[0,12,16],[1....|\n",
      "|  1|  1|(18,[0,7,16],[1.0...|\n",
      "|  2|  1|(18,[0,7,16],[1.0...|\n",
      "|  1|  1|(18,[0,13,16],[1....|\n",
      "|  1|  1|(18,[1,12,16],[1....|\n",
      "+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selection and test/train Split\n",
    "final_t = transformed_t.select('FS','FSL','features')\n",
    "final_t.show()\n",
    "train_t, test_t = final_t.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression vs GLM Gaussian vs GLM Poisson test\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='FSL')\n",
    "\n",
    "glrg = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", labelCol='FS', \n",
    "                                   featuresCol='features', maxIter=10, regParam=0.3, fitIntercept = True)\n",
    "\n",
    "glrp = GeneralizedLinearRegression(family=\"poisson\", link=\"log\", labelCol='FS', \n",
    "                                   featuresCol='features', maxIter=10, regParam=0.3, fitIntercept = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.5829316900397581\n"
     ]
    }
   ],
   "source": [
    "#Fitting and Evaluating Logistic Regression\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "model_lr = lr.fit(train_t)\n",
    "results_lr = model_lr.transform(test_t)\n",
    "eval_lr = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='FSL')\n",
    "\n",
    "summary_lr = model_lr.summary\n",
    "AUC = eval_lr.evaluate(results_lr)\n",
    "print(\"AUC:\" + str(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.6066056662916736\n",
      "Coefficients: [0.008173267972252929,-0.008173267972252853,-0.03292222194291687,-0.00479969758229749,-0.006605992753457887,-0.017860123348540972,0.01138630757600587,-0.013372948122147064,0.01208063423219443,0.015651129838158983,0.029857147598803386,0.008803853086716494,0.02274530104898066,-0.0015808928669626854,0.004452438336725364,0.08989853212993383,-0.18372236258616684,0.18372236258616648]\n",
      "Intercept: 0.40779289590689205\n",
      "Coefficient Standard Errors: [0.015572543527485423, 0.015572543527485423, 0.018306934485625037, 0.018890007369784936, 0.019115162267565825, 0.021827562090597438, 0.022189333221026775, 0.022895123955247, 0.023153033738146825, 0.024006909349673176, 0.024572415655036612, 0.025520418561882086, 0.025803006850404587, 0.027770147114092737, 0.03335051087208101, 0.051260799827331796, 0.02306396804543514, 0.023063968045435115, 0.029165808144352982]\n",
      "P Values: [0.5997032201784012, 0.5997032201784012, 0.072164976492475, 0.7994361047922021, 0.7296618549501619, 0.4132495324141974, 0.6078672375559213, 0.5591745610677274, 0.6018446633292664, 0.5144594699119283, 0.2243811575720005, 0.7301250722539319, 0.37807847569818254, 0.9546043441642142, 0.8937983501473936, 0.07951731543249707, 1.9984014443252818e-15, 1.9984014443252818e-15, 0.0]\n",
      "Dispersion: 0.33984921860781087\n",
      "Deviance: 2385.4016654082243\n",
      "AIC: 12398.168836807821\n"
     ]
    }
   ],
   "source": [
    "#Fitting and Evaluating GLM Gaussian\n",
    "\n",
    "model_glrg = glrg.fit(train_t)\n",
    "results_glrg = model_glrg.transform(test_t)\n",
    "eval_glrg = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='FS')\n",
    "\n",
    "summary_glrg = model_glrg.summary\n",
    "AUC = eval_glrg.evaluate(results_glrg)\n",
    "print(\"AUC:\" + str(AUC))\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model_glrg.coefficients))\n",
    "print(\"Intercept: \" + str(model_glrg.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"Coefficient Standard Errors: \" + str(summary_glrg.coefficientStandardErrors))\n",
    "#print(\"T Values: \" + str(summary_glrg.tValues))\n",
    "print(\"P Values: \" + str(summary_glrg.pValues))\n",
    "print(\"Dispersion: \" + str(summary_glrg.dispersion))\n",
    "#print(\"Null Deviance: \" + str(summary_glrg.nullDeviance))\n",
    "#print(\"Residual Degree Of Freedom Null: \" + str(summary_glrg.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary_glrg.deviance))\n",
    "#print(\"Residual Degree Of Freedom: \" + str(summary_glrg.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary_glrg.aic))\n",
    "#print(\"Deviance Residuals: \")\n",
    "#summary_glrg.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.6086519940846774\n",
      "Coefficients: [0.024199027970040175,-0.02419902796987876,-0.04199291535883224,-0.005570384912688892,-0.0025359562488697536,-0.021691595651703414,0.013002378973480493,-0.015565344812879614,0.008449973334513536,0.009315874946861594,0.024551961996780967,0.004012231803960074,0.013814326681536282,-0.005527130685534327,-8.452267313886252e-05,0.019821102606597803,-0.31950996706601564,0.31950996706614854]\n",
      "Intercept: -1.0920154089808356\n",
      "Coefficient Standard Errors: [0.03483425067039704, 0.03483425067039701, 0.035736154855385366, 0.03609069693853523, 0.036103461787580615, 0.03752519350542378, 0.03750080212866343, 0.037879058073075116, 0.03794537562595549, 0.0382583601803345, 0.03829327408230291, 0.03860154483021337, 0.03871720061446873, 0.039151041669655724, 0.03989961972592014, 0.040939431582935074, 0.03588609925720829, 0.03588609925720844, 0.050769377719240436]\n",
      "P Values: [0.48724919370518815, 0.48724919370809205, 0.2399618772285561, 0.8773384538675197, 0.944001555255136, 0.5632275282280506, 0.728799662647841, 0.6811296040236956, 0.8237784742538647, 0.8076188090447465, 0.5214213410180395, 0.9172172260369615, 0.7212409657748098, 0.8877320509380071, 0.9983097762344841, 0.6282746205280636, 0.0, 0.0, 0.0]\n",
      "Dispersion: 1.0\n",
      "Deviance: nan\n",
      "AIC: 9155.36315793917\n"
     ]
    }
   ],
   "source": [
    "#Fitting and Evaluating GLM Poisson\n",
    "\n",
    "model_glrp = glrp.fit(train_t)\n",
    "results_glrp = model_glrp.transform(test_t)\n",
    "eval_glrp = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='FS')\n",
    "\n",
    "summary_glrp = model_glrp.summary\n",
    "AUC = eval_glrp.evaluate(results_glrp)\n",
    "print(\"AUC:\" + str(AUC))\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model_glrp.coefficients))\n",
    "print(\"Intercept: \" + str(model_glrp.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"Coefficient Standard Errors: \" + str(summary_glrp.coefficientStandardErrors))\n",
    "#print(\"T Values: \" + str(summary_glrp.tValues))\n",
    "print(\"P Values: \" + str(summary_glrp.pValues))\n",
    "print(\"Dispersion: \" + str(summary_glrp.dispersion))\n",
    "#print(\"Null Deviance: \" + str(summary_glrp.nullDeviance))\n",
    "#print(\"Residual Degree Of Freedom Null: \" + str(summary_glrp.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary_glrp.deviance))\n",
    "#print(\"Residual Degree Of Freedom: \" + str(summary_glrp.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary_glrp.aic))\n",
    "#print(\"Deviance Residuals: \")\n",
    "#summary_glrp.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|    features2|\n",
      "+--------------------+-------------+\n",
      "|(18,[1,2,16],[1.0...|(1,[0],[1.0])|\n",
      "|(18,[1,11,16],[1....|(1,[0],[1.0])|\n",
      "|(18,[0,6,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,4,17],[1.0...|    (1,[],[])|\n",
      "|(18,[1,5,16],[1.0...|(1,[0],[1.0])|\n",
      "|(18,[1,4,17],[1.0...|(1,[0],[1.0])|\n",
      "|(18,[0,8,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,7,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,5,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,3,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,8,16],[1.0...|    (1,[],[])|\n",
      "|(18,[1,13,16],[1....|(1,[0],[1.0])|\n",
      "|(18,[0,6,17],[1.0...|    (1,[],[])|\n",
      "|(18,[1,6,17],[1.0...|(1,[0],[1.0])|\n",
      "|(18,[0,8,17],[1.0...|    (1,[],[])|\n",
      "|(18,[0,12,16],[1....|    (1,[],[])|\n",
      "|(18,[0,7,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,7,16],[1.0...|    (1,[],[])|\n",
      "|(18,[0,13,16],[1....|    (1,[],[])|\n",
      "|(18,[1,12,16],[1....|(1,[0],[1.0])|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ChiSqSelector output with top 1 features selected\n",
      "+---+---+--------------------+----------------+\n",
      "| FS|FSL|            features|selectedFeatures|\n",
      "+---+---+--------------------+----------------+\n",
      "|  3|  1|(18,[1,2,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  2|  1|(18,[1,11,16],[1....|   (1,[0],[1.0])|\n",
      "|  3|  1|(18,[0,6,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  6|  1|(18,[0,4,17],[1.0...|       (1,[],[])|\n",
      "|  1|  1|(18,[1,5,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[1,4,17],[1.0...|       (1,[],[])|\n",
      "|  1|  1|(18,[0,8,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,7,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,5,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  2|  1|(18,[0,3,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,8,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  3|  1|(18,[1,13,16],[1....|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,6,17],[1.0...|       (1,[],[])|\n",
      "|  1|  1|(18,[1,6,17],[1.0...|       (1,[],[])|\n",
      "|  1|  1|(18,[0,8,17],[1.0...|       (1,[],[])|\n",
      "|  2|  1|(18,[0,12,16],[1....|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,7,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  2|  1|(18,[0,7,16],[1.0...|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[0,13,16],[1....|   (1,[0],[1.0])|\n",
      "|  1|  1|(18,[1,12,16],[1....|   (1,[0],[1.0])|\n",
      "+---+---+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature selection method test\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "#Vector Slicer\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=[1])\n",
    "output = slicer.transform(final_t)\n",
    "output.select(\"features\", \"features2\").show()\n",
    "\n",
    "#ChiSq Selector\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"FS\")\n",
    "\n",
    "result = selector.fit(final_t).transform(final_t)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimmied data set with all features\n",
    "dft = df.limit(10000).drop('_c0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| FS|            features|\n",
      "+---+--------------------+\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "|  0|(228,[0,1,9,14,81...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Full Model (~1min)\n",
    "\n",
    "\n",
    "categorical_columns = ['CRASH_YEAR','MULTI_VEH','HOLIDAY','TLA_NAME','INTERSECTION','JUNCTION_TYPE',\n",
    "                      'INTSN_MIDBLOCK','FLAT_HILL','ROAD_CHARACTER','ROAD_CURVATURE','ROAD_MARKINGS',\n",
    "                      'ROAD_SURFACE','NUM_LANES','SPD_LIM','ADV_SPD','TMP_SPD_LIM','LIGHT',\n",
    "                      'STREET_LIGHT','WEATHER_A','WEATHER_B','Sex','Age','DUI']\n",
    "numerical_columns = ['ANIMALS','BRIDGE','CLIFF_BANK','DEBRIS','DITCH','FENCE','GUARD_RAIL','HOUSE_OR_BLDG',\n",
    "                     'KERB','OBJ_THROWN_DROPPED','OVER_BANK','PARKED_VEHICLE','PHONE_BOX_ETC','POST_OR_POLE',\n",
    "                     'ROADWORKS','SLIP_OR_FLOOD','STRAY_ANIMAL','TRAFFIC_ISLAND','TRAFFIC_SIGN','TRAIN',\n",
    "                     'TREE','VEHICLE','WATER_RIVER','BICYCLE','BUS','CAR_STN_WAGON','MOPED','MOTOR_CYCLE',\n",
    "                     'SCHOOL_BUS','SUV','TAXI','TRUCK','VAN_OR_UTILITY','PEDESTRIAN']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + numerical_columns,outputCol=\"features\")\n",
    "\n",
    "#Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "\n",
    "#Call to transform\n",
    "model_f = pipeline.fit(dft)\n",
    "transformed_f = model_f.transform(dft)\n",
    "\n",
    "#Selection and test/train Split\n",
    "final_f = transformed_f.select('FS','features')\n",
    "train_f, test_f = final_f.randomSplit([0.7,.3])\n",
    "train_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7671447485684412\n",
      "Intercept: -0.9550110032756095\n",
      "AIC: 8841.511578903093\n"
     ]
    }
   ],
   "source": [
    "#Fitting and Evaluating Full Model (~4min)\n",
    "\n",
    "model_glrp = glrp.fit(train_f)\n",
    "results_glrp = model_glrp.transform(test_f)\n",
    "eval_glrp = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='FS')\n",
    "\n",
    "summary_glrp = model_glrp.summary\n",
    "AUC = eval_glrp.evaluate(results_glrp)\n",
    "print(\"AUC:\" + str(AUC))\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "#print(\"Coefficients: \" + str(model_glrp.coefficients))\n",
    "print(\"Intercept: \" + str(model_glrp.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "#print(\"Coefficient Standard Errors: \" + str(summary_glrp.coefficientStandardErrors))\n",
    "#print(\"T Values: \" + str(summary_glrp.tValues))\n",
    "#print(\"P Values: \" + str(summary_glrp.pValues))\n",
    "#print(\"Dispersion: \" + str(summary_glrp.dispersion))\n",
    "#print(\"Null Deviance: \" + str(summary_glrp.nullDeviance))\n",
    "#print(\"Residual Degree Of Freedom Null: \" + str(summary_glrp.residualDegreeOfFreedomNull))\n",
    "#print(\"Deviance: \" + str(summary_glrp.deviance))\n",
    "#print(\"Residual Degree Of Freedom: \" + str(summary_glrp.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary_glrp.aic))\n",
    "#print(\"Deviance Residuals: \")\n",
    "#summary_glrp.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 25 features selected\n",
      "+---+--------------------+--------------------+\n",
      "| FS|            features|    selectedFeatures|\n",
      "+---+--------------------+--------------------+\n",
      "|  3|(228,[0,2,10,14,8...|(25,[1,4,6,9,12,1...|\n",
      "|  2|(228,[0,1,10,22,8...|(25,[0,7,8,10,13,...|\n",
      "|  3|(228,[0,1,10,27,8...|(25,[0,7,8,10,11,...|\n",
      "|  6|(228,[0,1,10,33,8...|(25,[0,7,8,10,13,...|\n",
      "|  1|(228,[0,1,9,79,82...|(25,[0,6,9,10,16,...|\n",
      "|  1|(228,[0,1,9,14,81...|(25,[0,4,7,8,11,1...|\n",
      "|  1|(228,[0,2,9,33,81...|(25,[1,7,8,13,19,...|\n",
      "|  1|(228,[0,3,9,14,81...|(25,[2,4,8,10,12,...|\n",
      "|  1|(228,[0,3,9,19,81...|(25,[2,7,8,10,12,...|\n",
      "|  2|(228,[0,2,9,14,81...|(25,[1,4,7,8,17,2...|\n",
      "|  1|(228,[0,4,9,32,81...|(25,[8,10,13,19,2...|\n",
      "|  3|(228,[0,2,9,50,81...|(25,[1,7,8,13,16,...|\n",
      "|  1|(228,[0,2,9,14,81...|(25,[1,4,7,8,13,1...|\n",
      "|  1|(228,[0,1,9,18,82...|(25,[0,6,9,17,21,...|\n",
      "|  1|(228,[0,1,9,14,81...|(25,[0,4,7,8,13,1...|\n",
      "|  2|(228,[0,1,9,14,82...|(25,[0,4,6,9,10,1...|\n",
      "|  1|(228,[0,3,9,14,82...|(25,[2,4,6,9,10,1...|\n",
      "|  2|(228,[0,1,9,29,81...|(25,[0,7,8,11,13,...|\n",
      "|  1|(228,[0,2,9,18,81...|(25,[1,7,8,13,16,...|\n",
      "|  1|(228,[0,2,9,14,81...|(25,[1,4,7,8,10,1...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ChiSq Selector\n",
    "selector = ChiSqSelector(numTopFeatures=25, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"FS\")\n",
    "\n",
    "result = selector.fit(final_f).transform(final_f)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()\n",
    "#result.show(20, False) #Full view, hard to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| FS|            features|\n",
      "+---+--------------------+\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "|  0|(82,[0,5,12,16,20...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7 Data Mining\n",
    "#Final Model (~2min)\n",
    "\n",
    "categorical_columns = ['HOLIDAY','JUNCTION_TYPE','ROAD_CHARACTER','ROAD_CURVATURE',\n",
    "                       'SPD_LIM','LIGHT','Sex','Age','DUI']\n",
    "\n",
    "#categorical_columns = ['TLA_NAME'] #Due to large number of levels it's best to run this seperatly\n",
    "\n",
    "numerical_columns = ['BRIDGE','CLIFF_BANK','DITCH','FENCE','OVER_BANK','POST_OR_POLE','TRAFFIC_SIGN',\n",
    "                     'TRAIN','TREE','BICYCLE','BUS','MOPED','MOTOR_CYCLE','WATER_RIVER','SUV',\n",
    "                     'TRUCK','VAN_OR_UTILITY','PEDESTRIAN']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + numerical_columns, outputCol=\"features\")\n",
    "\n",
    "#Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "\n",
    "#Call to transform\n",
    "model = pipeline.fit(df)\n",
    "transformed = model.transform(df)\n",
    "\n",
    "#Selection and test/train Split\n",
    "final = transformed.select('FS','features')\n",
    "train, test = final.randomSplit([0.9,.1])\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7471969567836736\n",
      "Coefficients: [-0.028136810422688348,0.01937140050436939,0.002011382699577969,0.0025027798476085616,0.004251247371048675,0.12704157106332828,-0.016162128133622874,-0.00594384881960432,-0.022519201123155832,-0.06297712807056198,-0.0136459432050499,-0.00579332170828007,0.010610041506717877,0.009096858865215544,-0.025580176163049133,0.005873275791373888,-0.13874619517755174,0.047364074844803476,0.08870028421536819,0.002681836121231098,-0.2668763781621775,0.3019734836597745,-0.006095590888973929,0.015970416715005136,-0.03145545337854414,-0.012112675115051595,-0.0067750900815662235,-0.002215249436427502,0.004069589469237835,-0.0012295933560959285,0.0012262496788667955,8.953881544077225e-05,0.0016623743350506882,0.00184364746234318,-3.567391834077336e-05,-2.7368754940059366e-05,-7.76909013807928e-05,8.664259947364489e-05,-3.196093093393474e-05,3.837905917560747e-05,-2.2258452612387222e-05,-5.338413763649706e-06,0.011914941342896598,-0.08070629673475369,0.06552591448381104,0.003265440910884834,0.013101774103402962,-0.01310177410026207,-0.024473913224957167,-0.013090914879181174,-0.016262504634651936,0.0045273684985748415,0.013256014405074023,-0.0014750690083137454,-0.0010604115732314346,0.008602534976592061,0.004876599801851677,0.0048807909931353945,0.011887421514409692,0.003366569268304179,0.0038499294718592225,0.001115584393244505,-0.3337065788707693,0.33370657887156424,0.018075413897581973,0.026011668119276313,0.028003641439809566,0.04229214052332709,0.04132948537577849,0.05216179085991247,0.006888103278411858,0.00783883978688213,0.10871243684508143,0.1353590803580473,0.009879252542211652,0.03520193837008327,0.3246909740690826,0.02252760755245175,0.010579897441433046,0.048936163923232454,0.04008393595155587,0.26124168206616627]\n",
      "Intercept: -2.2887205783414353\n",
      "Coefficient Standard Errors: [0.007747728443712231, 0.008080011974911443, 0.008248615941566393, 0.008283420504172566, 0.008296962879997512, 0.006686228479215151, 0.007126007371338852, 0.007517528357811376, 0.007724288671410948, 0.007987685668100333, 0.008255801718425819, 0.00832457538829272, 0.007972348570201356, 0.008140405305746423, 0.008256708027774244, 0.008383119373844663, 0.0067266484272647965, 0.007164459498222757, 0.007124166106419911, 0.007986784557058025, 0.0067380180154054045, 0.0067787965636972435, 0.007890203652018432, 0.008028541113296497, 0.00806342067442916, 0.0081387493656039, 0.008322463965702551, 0.008384384571355508, 0.00839507283500238, 0.008402799750446653, 0.00840505673307904, 0.00841126608249157, 0.008410137396002921, 0.00841415949516339, 0.008415495496053945, 0.008419403890672712, 0.008419490472182494, 0.008419585372221627, 0.00841968295396894, 0.008419680626971751, 0.008419723798170064, 0.008419795025740636, 0.0067004167214295635, 0.0068138191835730244, 0.006796710369738286, 0.007878566373477018, 0.0070150655210429935, 0.0070150655210429675, 0.007201710628000904, 0.0072630006305627, 0.007311572977992331, 0.0075003217863421, 0.007545524515999755, 0.0075859960283740846, 0.00761618638388407, 0.007688132961479445, 0.007757862934456613, 0.007803924159129829, 0.007839833709226599, 0.00788066879701051, 0.008055665791906641, 0.008268640928262589, 0.007368657872688793, 0.00736865787268828, 0.008314482950748759, 0.007754707455605222, 0.007772374276361096, 0.007383103886591396, 0.008075308760900615, 0.007784113009939508, 0.00815818042607259, 0.008409024044807329, 0.007821576000490984, 0.008074122550296782, 0.00824205607504684, 0.008341628335322584, 0.007719459263603006, 0.008337944490366353, 0.007320518230158488, 0.007393523746782926, 0.006783219743499563, 0.008007302963220777, 0.017784845671510664]\n",
      "P Values: [0.0002816469549573597, 0.016509769028453913, 0.8073509661103095, 0.7625428399636482, 0.6083809083534226, 0.0, 0.02332626954985173, 0.4291393298020143, 0.003552609403587148, 3.1086244689504383e-15, 0.09835297894639616, 0.48647267172782227, 0.18323666253270954, 0.26378291273541477, 0.0019476020222175539, 0.48354803497064136, 0.0, 3.8179237549229583e-11, 0.0, 0.7370336001848077, 0.0, 0.0, 0.4397876903471867, 0.04667854466753818, 9.579372986845414e-05, 0.1366791009020567, 0.4156033475908729, 0.7916170776548881, 0.6278471100932959, 0.8836597879831911, 0.8840048491989345, 0.9915065950260964, 0.8433086149021578, 0.8265624193251391, 0.9966177174673221, 0.9974063403014886, 0.9926376196687143, 0.9917894327008858, 0.996971255065334, 0.9963630505610928, 0.9978907081245829, 0.9994941162317144, 0.07536463432869, 0.0, 0.0, 0.6785288639467295, 0.06180914278654459, 0.06180914284899153, 0.0006779431928984891, 0.07148070845761545, 0.026134034341772416, 0.5460941616409194, 0.07895107046253491, 0.8458264241637139, 0.8892673712034345, 0.2631670975058842, 0.5296103558477345, 0.5316903316302426, 0.12944732677856985, 0.6692385078044136, 0.6327101785676794, 0.8926771001150666, 0.0, 0.0, 0.029707592460162413, 0.0007956408634752776, 0.000314600277840249, 1.0148202278514873e-08, 3.0878142709411804e-07, 2.0691670599148893e-11, 0.3984914086190301, 0.351236396966518, 0.0, 0.0, 0.23066821960637318, 2.4426766136453892e-05, 0.0, 0.006896152325142957, 0.14839080385323733, 3.621569710787753e-11, 3.4360894130003317e-09, 0.0, 0.0]\n",
      "Dispersion: 1.0\n",
      "AIC: 319857.97921779024\n"
     ]
    }
   ],
   "source": [
    "#Fitting and Evaluating Final Model (~15min with full dataset)\n",
    "\n",
    "model_glrp = glrp.fit(train)\n",
    "results_glrp = model_glrp.transform(test)\n",
    "eval_glrp = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='FS')\n",
    "\n",
    "summary_glrp = model_glrp.summary\n",
    "AUC = eval_glrp.evaluate(results_glrp)\n",
    "print(\"AUC:\" + str(AUC))\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model_glrp.coefficients))\n",
    "print(\"Intercept: \" + str(model_glrp.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"Coefficient Standard Errors: \" + str(summary_glrp.coefficientStandardErrors))\n",
    "print(\"P Values: \" + str(summary_glrp.pValues))\n",
    "print(\"Dispersion: \" + str(summary_glrp.dispersion))\n",
    "print(\"AIC: \" + str(summary_glrp.aic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
